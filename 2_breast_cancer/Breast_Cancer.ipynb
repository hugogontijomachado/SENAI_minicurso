{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Dataset Description </h5>\n",
    "The Data description is as follows:\n",
    "\n",
    "1. diagnosis: The diagnosis of breast tissues (1 = malignant, 0 = benign) where malignant denotes that the disease is harmful\n",
    "2. mean_radius: mean of distances from center to points on the perimeter\n",
    "3. mean_texture: standard deviation of gray-scale values\n",
    "4. mean_perimeter: mean size of the core tumor\n",
    "5. mean_area: mean area of the core tumor\n",
    "6. mean_smoothness: mean of local variation in radius lengths\n",
    "\n",
    "\n",
    "All feature values are recoded with four significant digits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('Breast_cancer_data.csv')\n",
    "data = pd.read_csv('data.csv')\n",
    "data = data.drop('id', axis=1)\n",
    "data['diagnosis'] = data['diagnosis'].map({'M':1, 'B':0})\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checando se existem valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check null entries\n",
    "data.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removendo ítens duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove duplicate entries\n",
    "data.drop_duplicates(inplace = True)\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porque temos Raio, Perímetro e Área como features independetes? Não são features correlacionadas? Vamos investigar!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perímetro X Raio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlação entre o perímetro médio e o raio médio: {data['perimeter_mean'].corr(data['radius_mean'])}\")\n",
    "\n",
    "plt.scatter(data.perimeter_mean, data.radius_mean)\n",
    "plt.xlabel(\"Perímetro Médio\")\n",
    "plt.ylabel(\"Raio Médio\")\n",
    "plt.title(\"Relação entre Perímetro Médio e Raio Médio\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Área x Raio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlação entre o área média e o raio médio: {data['area_mean'].corr(data['radius_mean'])}\")\n",
    "\n",
    "plt.scatter(data.area_mean, data.radius_mean)\n",
    "plt.xlabel(\"Área Média\")\n",
    "plt.ylabel(\"Raio Médio\")\n",
    "plt.title(\"Relação entre Área Média e Raio Médio\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perímetro x Área"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlação entre o perímetro médio e o área média: {data['perimeter_mean'].corr(data['area_mean'])}\")\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(data.perimeter_mean, data.area_mean)\n",
    "plt.xlabel(\"Perímetro Médio\")\n",
    "plt.ylabel(\"Área Média\")\n",
    "plt.title(\"Relação entre Perímetro Médio e Área Média\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos então descartar a coluna \"Perímetro\" e a coluna \"Área\"\n",
    "- Estas duas colunas não complementam a variabilidade dos dados e só dificultarão o treinamento do modelo\n",
    "- Retirando essas colunas nós também retiramos a necessidade de obtê-las para uma inferência futura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['perimeter_mean'], axis=1, inplace=True)\n",
    "data.drop(['area_mean'], axis=1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "#X_train = sc.fit_transform(X_train)\n",
    "#X_test = sc.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tudo certo! Agora vamos separar os dados de treinamento e de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.iloc[:,1:].values\n",
    "y = data.iloc[:,0].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0)\n",
    "\n",
    "print(f\"O conjunto de treinamento contém {len(X_train)} ítens e o conjunto de teste contém {len(X_test)} itens.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tudo certo mesmo? Vamos dar uma olhada nos valores médios de cada coluna para tentar entender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos realizar uma trasnformação nos dados para que fiquem com a média próxima de zero e amplitudes mais próximas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train, columns = data.columns[1:]).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agora sim, tudo pronto!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos instanciar as métricas para podermos avaliar os modelos posteriormente. Relembrando:\n",
    "\n",
    "- <span style=\"color: green\">Acertos</span>\n",
    "    - \"True Positive\" (TP): Previsão --> <span style=\"color: orange\">Maligno</span> ;  Real --> <span style=\"color: orange\">Maligno</span>\n",
    "    - \"True Negative\" (TN): Previsão --> <span style=\"color: cyan\">Benigno</span> ;  Real --> <span style=\"color: cyan\">Benigno</span>\n",
    "<br><br>\n",
    "- <span style=\"color: red\">Erros</span>\n",
    "    - \"False Positive\" (FP): Previsão --> <span style=\"color: orange\">Maligno</span> ;  Real --> <span style=\"color: cyan\">Benigno</span>\n",
    "    - \"False Negative\" (FN): Previsão --> <span style=\"color: cyan\">Benigno</span> ;  Real --> <span style=\"color: orange\">Maligno</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas:\n",
    "\n",
    "- Acurácia = $\\frac{T_P + T_N} {T_P + T_N + F_P + F_N}$\n",
    "<br><br>\n",
    "- Recall = $\\frac{T_P}{T_P+F_N}$\n",
    "<br><br>\n",
    "- Precisão = $\\frac{T_P}{T_P+F_P}$\n",
    "<br><br>\n",
    "- F1 - Score = $\\frac{{2P_{rec}R_{ec}}}{{P_{rec}+R_{ec}}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "acc = {}\n",
    "rec = {}\n",
    "prec = {}\n",
    "f1 = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão linear\n",
    "<img src=\"./images/linear_regression.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o módulo de regressão linear \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Treinando o modelo \n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Prevendo os resultados\n",
    "y_pred = regr.predict(X_test)\n",
    "y_pred = [1 if x >= 0.5 else 0 for x in y_pred]\n",
    "\n",
    "\n",
    "# Analisando a qualidade do modelo\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "acc['reg_linear'] = accuracy_score(y_test, y_pred)\n",
    "rec['reg_linear'] = recall_score(y_test, y_pred)\n",
    "prec['reg_linear'] = precision_score(y_test, y_pred)\n",
    "f1['reg_linear'] = f1_score(y_test, y_pred)\n",
    "print(f\"Acurácia: {100*acc['reg_linear']} %\")\n",
    "print(f\"Recall: {100*rec['reg_linear']} %\")\n",
    "print(f\"Precisão: {100*prec['reg_linear']} %\")\n",
    "print(f\"F1: {100*f1['reg_linear']} %\")\n",
    "\n",
    "print(f\"\\n\\nMatriz de confusão:\")\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.set(font_scale = 1)\n",
    "sns.set_style(\"white\")\n",
    "_ = sns.heatmap(cm, cmap = 'gist_yarg_r',annot = True, fmt='d')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão logística\n",
    "<img src=\"./images/logistic.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando modelo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Prevendo os resultados\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Analisando a qualidade do modelo\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "acc['logistic'] = accuracy_score(y_test, y_pred)\n",
    "rec['logistic'] = recall_score(y_test, y_pred)\n",
    "prec['logistic'] = precision_score(y_test, y_pred)\n",
    "f1['logistic'] = f1_score(y_test, y_pred)\n",
    "print(f\"Acurácia: {100*acc['logistic']} %\")\n",
    "print(f\"Recall: {100*rec['logistic']} %\")\n",
    "print(f\"Precisão: {100*prec['logistic']} %\")\n",
    "print(f\"F1: {100*f1['logistic']} %\")\n",
    "\n",
    "\n",
    "print(f\"\\n\\nMatriz de confusão:\")\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.set(font_scale = 1)\n",
    "sns.set_style(\"white\")\n",
    "_ = sns.heatmap(cm, cmap = 'gist_yarg_r',annot = True, fmt='d')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Neighbors (vizinho mais próximo)\n",
    "<img src=\"./images/k_nei.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando modelo\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Prevendo os resultados\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Analisando a qualidade do modelo\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "acc['k_neighbors'] = accuracy_score(y_test, y_pred)\n",
    "rec['k_neighbors'] = recall_score(y_test, y_pred)\n",
    "prec['k_neighbors'] = precision_score(y_test, y_pred)\n",
    "f1['k_neighbors'] = f1_score(y_test, y_pred)\n",
    "print(f\"Acurácia: {100* acc['k_neighbors']} %\")\n",
    "print(f\"Recall: {100* rec['k_neighbors']} %\")\n",
    "print(f\"Precisão: {100* prec['k_neighbors']} %\")\n",
    "print(f\"F1: {100* f1['k_neighbors']} %\")\n",
    "\n",
    "\n",
    "print(f\"\\n\\nMatriz de confusão:\")\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.set(font_scale = 1)\n",
    "sns.set_style(\"white\")\n",
    "_ = sns.heatmap(cm, cmap = 'gist_yarg_r',annot = True, fmt='d')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM \n",
    "<img src=\"./images/SVM_linear.png\" width=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando modelo\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel = 'linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Prevendo os resultados\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "\n",
    "# Analisando a qualidade do modelo\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "acc['SVM_linear'] = accuracy_score(y_test, y_pred)\n",
    "rec['SVM_linear'] = recall_score(y_test, y_pred)\n",
    "prec['SVM_linear'] = precision_score(y_test, y_pred)\n",
    "f1['SVM_linear'] = f1_score(y_test, y_pred)\n",
    "print(f\"Acurácia: {100*acc['SVM_linear']} %\")\n",
    "print(f\"Recall: {100*rec['SVM_linear']} %\")\n",
    "print(f\"Precisão: {100*prec['SVM_linear']} %\")\n",
    "print(f\"F1: {100*f1['SVM_linear']} %\")\n",
    "\n",
    "\n",
    "print(f\"\\n\\nMatriz de confusão:\")\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.set(font_scale = 1)\n",
    "sns.set_style(\"white\")\n",
    "_ = sns.heatmap(cm, cmap = 'gist_yarg_r',annot = True, fmt='d')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernal SVM\n",
    "<img src=\"./images/SVM_kernal.png\" width=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando modelo\n",
    "from sklearn.svm import SVC\n",
    "kernel_svm = SVC(kernel = 'rbf')\n",
    "kernel_svm.fit(X_train, y_train)\n",
    "\n",
    "# Prevendo os resultados\n",
    "y_pred = kernel_svm.predict(X_test)\n",
    "\n",
    "\n",
    "# Analisando a qualidade do modelo\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "acc['SVM_kernal'] = accuracy_score(y_test, y_pred)\n",
    "rec['SVM_kernal'] = recall_score(y_test, y_pred)\n",
    "prec['SVM_kernal'] = precision_score(y_test, y_pred)\n",
    "f1['SVM_kernal'] = f1_score(y_test, y_pred)\n",
    "print(f\"Acurácia: {100*acc['SVM_kernal']} %\")\n",
    "print(f\"Recall: {100*rec['SVM_kernal']} %\")\n",
    "print(f\"Precisão: {100*prec['SVM_kernal']} %\")\n",
    "print(f\"F1: {100*f1['SVM_kernal']} %\")\n",
    "\n",
    "\n",
    "print(f\"\\n\\nMatriz de confusão:\")\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.set(font_scale = 1)\n",
    "sns.set_style(\"white\")\n",
    "_ = sns.heatmap(cm, cmap = 'gist_yarg_r',annot = True, fmt='d')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes (Redes Bayesianas)\n",
    "<img src=\"./images/naive.png\" width=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando modelo\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Prevendo os resultados\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "\n",
    "# Analisando a qualidade do modelo\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "acc['naive_bayes'] = accuracy_score(y_test, y_pred)\n",
    "rec['naive_bayes'] = recall_score(y_test, y_pred)\n",
    "prec['naive_bayes'] = precision_score(y_test, y_pred)\n",
    "f1['naive_bayes'] = f1_score(y_test, y_pred)\n",
    "print(f\"Acurácia: {100*acc['naive_bayes']} %\")\n",
    "print(f\"Recall: {100*rec['naive_bayes']} %\")\n",
    "print(f\"Precisão: {100*prec['naive_bayes']} %\")\n",
    "print(f\"F1: {100*f1['naive_bayes']} %\")\n",
    "\n",
    "\n",
    "print(f\"\\n\\nMatriz de confusão:\")\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.set(font_scale = 1)\n",
    "sns.set_style(\"white\")\n",
    "_ = sns.heatmap(cm, cmap = 'gist_yarg_r',annot = True, fmt='d')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier (árvore de decisão)\n",
    "<img src=\"./images/tree.png\" width=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando modelo\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Prevendo os resultados\n",
    "y_pred = dtc.predict(X_test)\n",
    "\n",
    "# Analisando a qualidade do modelo\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "acc['tree'] = accuracy_score(y_test, y_pred)\n",
    "rec['tree'] = recall_score(y_test, y_pred)\n",
    "prec['tree'] = precision_score(y_test, y_pred)\n",
    "f1['tree'] = f1_score(y_test, y_pred)\n",
    "print(f\"Acurácia: {100*acc['tree']} %\")\n",
    "print(f\"Recall: {100*rec['tree']} %\")\n",
    "print(f\"Precisão: {100*prec['tree']} %\")\n",
    "print(f\"F1: {100*f1['tree']} %\")\n",
    "\n",
    "\n",
    "print(f\"\\n\\nMatriz de confusão:\")\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.set(font_scale = 1)\n",
    "sns.set_style(\"white\")\n",
    "_ = sns.heatmap(cm, cmap = 'gist_yarg_r',annot = True, fmt='d')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Random Forest\n",
    "<img src=\"./images/random_forest.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando modelo\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Prevendo os resultados\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Analisando a qualidade do modelo\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "acc['random_forest'] = accuracy_score(y_test, y_pred)\n",
    "rec['random_forest'] = recall_score(y_test, y_pred)\n",
    "prec['random_forest'] = precision_score(y_test, y_pred)\n",
    "f1['random_forest'] = f1_score(y_test, y_pred)\n",
    "print(f\"Acurácia: {100*acc['random_forest']} %\")\n",
    "print(f\"Recall: {100*rec['random_forest']} %\")\n",
    "print(f\"Precisão: {100*prec['random_forest']} %\")\n",
    "print(f\"F1: {100*f1['random_forest']} %\")\n",
    "\n",
    "\n",
    "print(f\"\\n\\nMatriz de confusão:\")\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.set(font_scale = 1)\n",
    "sns.set_style(\"white\")\n",
    "_ = sns.heatmap(cm, cmap = 'gist_yarg_r',annot = True, fmt='d')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network (ANN)\n",
    "<img src=\"./images/NN.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando módulos\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# Criando modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(units = 3, activation = 'relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units = 3, activation = 'relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compilando modelo\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "deep_history = model.fit(X_train, y_train, epochs=100, batch_size = 32, validation_data = (X_test, y_test), callbacks=[early_stop]                            )\n",
    "\n",
    "model_path = 'modelo.h5'\n",
    "model.save(model_path)\n",
    "\n",
    "model_weights_path= 'modelo_weights.h5'\n",
    "model.save_weights(model_weights_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model\n",
    "#model.load_weights(top_model_weights_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    historydata = pd.DataFrame(history.history, index=history.epoch)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    historydata.plot(ylim=(0, 1.01*historydata.values.max()))\n",
    "    plt.title('Loss: %.3f' % history.history['loss'][-1])\n",
    "plot_loss(deep_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevendo os resultados\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred>0.5)\n",
    "\n",
    "\n",
    "# Analisando a qualidade do modelo\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "acc['ANN'] = accuracy_score(y_test, y_pred)\n",
    "rec['ANN'] = recall_score(y_test, y_pred)\n",
    "prec['ANN'] = precision_score(y_test, y_pred)\n",
    "f1['ANN'] = f1_score(y_test, y_pred)\n",
    "print(f\"Acurácia: {100*acc['ANN']} %\")\n",
    "print(f\"Recall: {100*rec['ANN']} %\")\n",
    "print(f\"Precisão: {100*prec['ANN']} %\")\n",
    "print(f\"F1: {100*f1['ANN']} %\")\n",
    "\n",
    "\n",
    "print(f\"\\n\\nMatriz de confusão:\")\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.set(font_scale = 1)\n",
    "sns.set_style(\"white\")\n",
    "_ = sns.heatmap(cm, cmap = 'gist_yarg_r',annot = True, fmt='d')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Relembrando:\n",
    "- <span style=\"color: green\">Acertos</span>\n",
    "    - \"True Positive\" (TP): Previsão --> <span style=\"color: orange\">Maligno</span> ;  Real --> <span style=\"color: orange\">Maligno</span>\n",
    "    - \"True Negative\" (TN): Previsão --> <span style=\"color: cyan\">Benigno</span> ;  Real --> <span style=\"color: cyan\">Benigno</span>\n",
    "<br><br>\n",
    "- <span style=\"color: red\">Erros</span>\n",
    "    - \"False Positive\" (FP): Previsão --> <span style=\"color: orange\">Maligno</span> ;  Real --> <span style=\"color: cyan\">Benigno</span>\n",
    "    - \"False Negative\" (FN): Previsão --> <span style=\"color: cyan\">Benigno</span> ;  Real --> <span style=\"color: orange\">Maligno</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Acurácia = $\\frac{T_P + T_N} {T_P + T_N + F_P + F_N}$\n",
    "<br><br>\n",
    "- Recall = $\\frac{T_P}{T_P+F_N}$\n",
    "<br><br>\n",
    "- Precisão = $\\frac{T_P}{T_P+F_P}$\n",
    "<br><br>\n",
    "- F1 - Score = $\\frac{{2P_{rec}R_{ec}}}{{P_{rec}+R_{ec}}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame({\n",
    "    'Modelo': acc.keys(),\n",
    "    'Acurácia':  np.array(list(acc.values()))*100,\n",
    "    'Recall': np.array(list(rec.values()))*100,\n",
    "    'Precisão': np.array(list(prec.values()))*100,\n",
    "    'F1': np.array(list(f1.values()))*100\n",
    "}).sort_values(by = ['Recall'], ascending = False)\n",
    "metrics.reset_index(drop = True, inplace = True)\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
